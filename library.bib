Automatically generated by Mendeley Desktop 1.13.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Buades2005,
abstract = {The search for efficient image denoising methods is still a valid challenge at the crossing of functional analysis and statistics. In spite of the sophistication of the recently proposed methods, most algorithms have not yet attained a desirable level of applicability. All show an outstanding performance when the image model corresponds to the algorithm assumptions but fail in general and create artifacts or remove image fine structures. The main focus of this paper is, first, to define a general mathematical and experimental methodology to compare and classify classical image denoising algorithms and, second, to propose a nonlocal means (NL-means) algorithm addressing the preservation of structure in a digital image. The mathematical analysis is based on the analysis of the "method noise," defined as the difference between a digital image and its denoised version. The NL-means algorithm is proven to be asymptotically optimal under a generic statistical image model. The denoising performance of all consid...},
author = {Buades, A. and Coll, B. and Morel, J. M.},
booktitle = {Multiscale Modeling \& Simulation},
isbn = {0769523722},
issn = {1540-3459},
pages = {490--530},
pmid = {231665400006},
title = {{A Review of Image Denoising Algorithms, with a New One}},
volume = {4},
year = {2005}
}
@article{Oren-Grinberg2013,
abstract = {OBJECTIVE: Portable ultrasound is now used routinely in many ICUs for various clinical applications. Echocardiography performed by noncardiologists, both transesophageal and transthoracic, has evolved to broad applications in diagnosis, monitoring, and management of critically ill patients. This review provides a current update on focused critical care echocardiography for the management of critically ill patients.$\backslash$n$\backslash$nMETHOD: Source data were obtained from a PubMed search of the medical literature, including the PubMed "related articles" search methodology.$\backslash$n$\backslash$nSUMMARY AND CONCLUSIONS: Although studies demonstrating improved clinical outcomes for critically ill patients managed by focused critical care echocardiography are generally lacking, there is evidence to suggest that some intermediate outcomes are improved. Furthermore, noncardiologists can learn focused critical care echocardiography and adequately interpret the information obtained. Noncardiologists can also successfully incorporate focused critical care echocardiography into advanced cardiopulmonary life support. Formal training and proctoring are important for safe application of focused critical care echocardiography in clinical practice. Further outcomes-based research is urgently needed to evaluate the efficacy of focused critical care echocardiography.},
author = {Oren-Grinberg, Achikam and Talmor, Daniel and Brown, Samuel M},
doi = {10.1097/CCM.0b013e31829e4dc5},
issn = {1530-0293},
journal = {Critical care medicine},
keywords = {Cardiopulmonary Resuscitation,Critical Care,Critical Care: methods,Echocardiography,Echocardiography: methods,Humans,Monitoring,Physiologic,Point-of-Care Systems,Transesophageal},
pages = {2618--26},
pmid = {23989172},
title = {{Focused critical care echocardiography.}},
volume = {41},
year = {2013}
}
@inproceedings{Chittajallu2010,
abstract = {In this paper, we present a knowledge-driven Markov Random Field (MRF) model for the segmentation of organs in medical images with particular emphasis on the incorporation of shape constraints into the segmentation problem. We cast the problem of image segmentation as the Maximum A Posteriori (MAP) estimation of a Markov Random Field which, in essence, is equivalent to the minimization of the corresponding Gibbs energy function. We then incorporate a set of constraints into the Gibbs energy function that collectively force the resulting segmentation contour/surface to have a shape similar to that of a given shape template. In particular, we introduce a flux-maximization constraint and a generalized template-based star-shape constraint that are encoded into the first- and second-order clique potentials of the Gibbs energy function, respectively. Our main contribution is in the translation of a set of global notions about the shape of the desired segmentation contour into a set of local measures that can be conveniently encoded into the Gibbs energy function and used in combination with other traditionally used constraints derived from image information. In our experiments, we demonstrate the application of the proposed method to the challenging problem of heart segmentation in non-contrast computed tomography (CT) data.},
author = {Chittajallu, D. R. and Shah, S. K. and Kakadiaris, I. A.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3233--3240},
title = {{A shape-driven MRF model for the segmentation of organs in medical images}},
year = {2010}
}
@article{Brunner2010,
abstract = {Measurements related to coronary artery calcification (CAC) offer significant predictive value for coronary artery disease (CAD). In current medical practice CAC scoring is a labor-intensive task. The objective of this paper is the development and evaluation of a family of coronary artery region (CAR) models applied to the detection of CACs in coronary artery zones and sections. Thirty patients underwent non-contrast electron-beam computed tomography scanning. Coronary artery trajectory points as presented in the University of Houston heart-centered coordinate system were utilized to construct the CAR models which automatically detect coronary artery zones and sections. On a per-patient and per-zone basis the proposed CAR models detected CACs with a sensitivity, specificity and accuracy of 85.56 (± 15.80)\%, 93.54 (± 1.98)\%, and 85.27 (± 14.67)\%, respectively while the corresponding values in the zones and segments based case were 77.94 (± 7.78)\%, 96.57 (± 4.90)\%, and 73.58 (± 8.96)\%, respectively. The results of this study suggest that the family of CAR models provide an effective method to detect different regions of the coronaries. Further, the CAR classifiers are able to detect CACs with a mean sensitivity and specificity of 86.33 and 93.78\%, respectively.},
author = {Brunner, Gerd and Chittajallu, Deepak R and Kurkure, Uday and Kakadiaris, Ioannis A},
doi = {10.1007/s10554-010-9608-1},
issn = {1875-8312},
journal = {The international journal of cardiovascular imaging},
pages = {829--838},
pmid = {20232154},
title = {{Toward the automatic detection of coronary artery calcification in non-contrast computed tomography data.}},
volume = {26},
year = {2010}
}
@article{Cleveland1988,
abstract = {Abstract Locally weighted regression, or loess, is a way of estimating a regression surface through a multivariate smoothing procedure, fitting a function of the independent variables locally and in a moving fashion analogous to how a moving average is computed for a time series. With local fitting we can estimate a much wider class of regression surfaces than with the usual classes of parametric functions, such as polynomials. The goal of this article is to show, through applications, how loess can be used for three purposes: data exploration, diagnostic checking of parametric models, and providing a nonparametric regression surface. Along the way, the following methodology is introduced: (a) a multivariate smoothing procedure that is an extension of univariate locally weighted regression; (b) statistical procedures that are analogous to those used in the least-squares fitting of parametric functions; (c) several graphical methods that are useful tools for understanding loess estimates and checking the assumptions on which the estimation procedure is based; and (d) the M plot, an adaptation of Mallows's Cp procedure, which provides a graphical portrayal of the trade-off between variance and bias, and which can be used to choose the amount of smoothing.},
annote = {doi: 10.1080/01621459.1988.10478639},
author = {Cleveland, William S and Devlin, Susan J},
doi = {10.1080/01621459.1988.10478639},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = sep,
number = {403},
pages = {596--610},
publisher = {Taylor \& Francis},
title = {{Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting}},
volume = {83},
year = {1988}
}
@incollection{Goshtasby2012,
abstract = {The topics of similarity and dissimilarity measures are discussed in detail. The chapter starts with definitions of similarity and dissimilarity measures and lists the requirements for them to be metrics. In addition to the existing similarity and dissimilarity measures, 3 new similarity measures and 1 new dissimilarity measure are introduced. The performances of 16 similarity measures and 10 dissimilarity measures in image matching are determined and compared, and their sensitivities to noise and blurring as well as to intensity and geometric changes are also determined and compared. The similarity measures tested are Pearson correlation, Tanimoto measure, stochastic sign change, deterministic sign change, minimum ratio, Spearman’s $\rho$, Kendall’s $\tau$, greatest deviation, ordinal measure, correlation ratio, energy of joint probability density, material similarity, Shannon mutual information, R\'{e}nyi mutual information, Tsallis mutual information, and I $\alpha$ information. The dissimilarity measures tested are L 1 norm, median of absolute differences, square L 2 norm, median of square differences, normalized square L 2 norm, incremental sign distance, intensity-ratio variance, intensity-mapping-ratio variance, rank distance, joint entropy, and exclusive F-information.},
author = {Goshtasby, A.Ardeshir},
booktitle = {Image Registration SE - 2},
doi = {10.1007/978-1-4471-2458-0\_2},
isbn = {978-1-4471-2457-3},
language = {English},
pages = {7--66},
publisher = {Springer London},
series = {Advances in Computer Vision and Pattern Recognition},
title = {{Similarity and Dissimilarity Measures}},
year = {2012}
}
@article{Bouwmans2014,
abstract = {Foreground detection is the first step in video surveillance system to detect moving objects. Recent research on subspace estimation by sparse representation and rank minimization represents a nice framework to separate moving objects from the background. Robust Principal Component Analysis (RPCA) solved via Principal Component Pursuit decomposes a data matrix A in two components such that A=L+S, where L is a low-rank matrix and S is a sparse noise matrix. The background sequence is then modeled by a low-rank subspace that can gradually change over time, while the moving foreground objects constitute the correlated sparse outliers. To date, many efforts have been made to develop Principal Component Pursuit (PCP) methods with reduced computational cost that perform visually well in foreground detection. However, no current algorithm seems to emerge and to be able to simultaneously address all the key challenges that accompany real-world videos. This is due, in part, to the absence of a rigorous quantitative evaluation with synthetic and realistic large-scale dataset with accurate ground truth providing a balanced coverage of the range of challenges present in the real world. In this context, this work aims to initiate a rigorous and comprehensive review of RPCA-PCP based methods for testing and ranking existing algorithms for foreground detection. For this, we first review the recent developments in the field of RPCA solved via Principal Component Pursuit. Furthermore, we investigate how these methods are solved and if incremental algorithms and real-time implementations can be achieved for foreground detection. Finally, experimental results on the Background Models Challenge (BMC) dataset which contains different synthetic and real datasets show the comparative performance of these recent methods. ?? 2013 Elsevier Inc. All rights reserved.},
author = {Bouwmans, Thierry and Zahzah, El Hadi},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Foreground detection,Principal Component Pursuit,Robust principal component analysis},
pages = {22--34},
title = {{Robust PCA via Principal Component Pursuit: A review for a comparative evaluation in video surveillance}},
volume = {122},
year = {2014}
}
@article{Weekes2011,
abstract = {Emergency echocardiography refers to the use of cardiac ultrasound to address critical and time-sensitive clinical questions during the initial evaluation and treatment of the critically ill patient presenting to the emergency department. The information obtained can be pivotal to a physician's clinical decision making and can guide further diagnostic or therapeutic interventions. This article provides an evidence-based discussion of the common uses of emergency transthoracic echocardiography, as well as its benefits and limitations in the current practice of emergency medicine. ?? 2011 Elsevier Inc.},
author = {Weekes, Anthony J. and Quirke, Dale P.},
doi = {10.1016/j.emc.2011.08.002},
isbn = {0733-8627},
issn = {07338627},
journal = {Emergency Medicine Clinics of North America},
keywords = {Cardiac ultrasound,Emergency echocardiography,Emergency medicine,Focused cardiac ultrasound,Point-of-care cardiac ultrasound},
pages = {759--787},
pmid = {1732097},
title = {{Emergency echocardiography}},
volume = {29},
year = {2011}
}
@inproceedings{Chittajallu2009,
abstract = {Image segmentation is, in general, an ill-posed problem and additional constraints need to be imposed in order to achieve the desired result. Particularly in the field of medical image segmentation, a significant amount of prior knowledge is available that can be used to constrain the solution space of the segmentation problem. However, most of this prior knowledge is, in general, vague or imprecise in nature, which makes it very difficult to model. This is the problem that is addressed in this paper. Specifically, in this paper, we present fuzzy-cuts, a novel, knowledge-driven, graph-based method for medical image segmentation. We cast the problem of image segmentation as the maximum a posteriori (MAP) estimation of a Markov random field (MRF) which, in essence, is equivalent to the minimization of the corresponding Gibbs energy function. Considering the inherent imprecision that is common in the a priori description of objects in medical images, we propose a fuzzy theoretic model to incorporate knowledge-driven constraints into the MAP-MRF formulation. In particular, we focus on prior information about the object's location, appearance and spatial connectivity to a known seed region inside the object. To that end, we introduce fuzzy connectivity and fuzzy location priors that are used in combination to define the first-order clique potential of the Gibbs energy function. In our experiments, we demonstrate the application of the proposed method to the challenging problem of heart segmentation in non-contrast computed tomography (CT) data.},
author = {Chittajallu, D. R. and Brunner, G. and Kurkure, U. and Yalamanchili, R. P. and Kakadiaris, I. A.},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
doi = {10.1109/CVPRW.2009.5206623},
isbn = {9781424439935},
issn = {1063-6919},
pages = {715--722},
title = {{Fuzzy-Cuts: A knowledge-driven graph-based method for medical image segmentation}},
year = {2009}
}
@article{Kuklik2015,
abstract = {The Hilbert transform has been used to characterize wave propagation and detect phase singularities during cardiac fibrillation. Two mapping modalities have been used: optical mapping (used to map atria and ventricles) and contact electrode mapping (used only to map ventricles). Due to specific morphology of atrial electrograms, phase reconstruction of contact electrograms in the atria is challenging and has not been investigated in detail. Here, we explore the properties of Hilbert transform applied to unipolar epicardial electrograms and devise a method for robust phase reconstruction using the Hilbert transform. We applied the Hilbert transform to idealized unipolar signals obtained from analytical approach and to electrograms recorded in humans. We investigated effects of deflection morphology on instantaneous phase. Application of the Hilbert transform to unipolar electrograms demonstrated sensitivity of reconstructed phase to the type of deflection morphology (uni- or biphasic), the ratio of R and S waves and presence of the noise. In order to perform a robust phase reconstruction, we propose a signal transformation based on the recomposition of the electrogram from sinusoidal wavelets with amplitudes proportional to the negative slope of the electrogram. Application of the sinusoidal recomposition transformation prior to application of the Hilbert transform alleviates the effect of confounding features on reconstructed phase.},
author = {Kuklik, Pawel and Zeemering, Stef and Maesen, Bart and Maessen, Jos and Crijns, Harry J and Verheule, Sander and Ganesan, Anand N and Schotten, Ulrich},
doi = {10.1109/TBME.2014.2350029},
issn = {1558-2531 (Electronic)},
journal = {IEEE transactions on bio-medical engineering},
language = {eng},
month = jan,
number = {1},
pages = {296--302},
pmid = {25148659},
title = {{Reconstruction of instantaneous phase of unipolar atrial contact electrogram using a concept of sinusoidal recomposition and Hilbert transform.}},
volume = {62},
year = {2015}
}
@article{Laptev2005,
abstract = { A method for detecting and segmenting periodic motion is presented. We exploit periodicity as a cue and detect periodic motion in complex scenes where common methods for motion segmentation are likely to fail. We note that periodic motion detection can be seen as an approximate case of sequence alignment where an image sequence is matched to itself over one or more periods of time. To use this observation, we first consider alignment of two video sequences obtained by independently moving cameras. Under assumption of constant translation, the fundamental matrices and the homographies are shown to be time-linear matrix functions. These dynamic quantities can be estimated by matching corresponding space-time points with similar local motion and shape. For periodic motion, we match corresponding points across periods and develop a RANSAC procedure to simultaneously estimate the period and the dynamic geometric transformations between periodic views. Using this method, we demonstrate detection and segmentation of human periodic motion in complex scenes with nonrigid backgrounds, moving camera and motion parallax.},
author = {Laptev, I. and Belongie, S.J. and Perez, P. and Wills, J.},
journal = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
title = {{Periodic motion detection and segmentation via approximate sequence alignment}},
volume = {1},
year = {2005}
}
@article{Chen2010,
abstract = {Quantitative analysis of breast morphometry is critical to breast plastic surgery. Recently, three-dimensional (3D) photography has emerged as a strong new alternative for breast morphometry analysis in comparison to other existing techniques. 3D photography enables the capture of the entire breast surface topology virtually in a single snapshot and without any direct contact with the patient, thus causing minimal discomfort. In this paper, we present a set of computational tools for the quantitative analysis of two key morphological properties of the breast that are of interest to breast plastic surgery based on 3D scans, namely breast shape and volume. The breast shape is modeled using a compact geometric model capable of capturing the global shape of the breast with very few parameters. Specifically, the shape model is deduced by applying a set of five global deformations to a geometric primitive. These deformations, defined using very intuitive parameters, closely model the key shape variables that surgeons inherently use to describe the overall shape of the breast. Patient-specific parameters of the breast shape model are automatically recovered by fitting a generic breast shape model to the 3D scan of the patient's breast using a physics-based deformable model framework. The mean error of fit between the automatically fitted shape model and the actual breast surface for 12 subjects varied between 0.9 and 2.6 mm. These results are very encouraging considering the fact that only 17 parameters are used to determine the shape of the breast. The breast volume is estimated automatically by first localizing the breast on a 3D scan of the patient's torso and then computing the volume enclosed between an interpolated breast-less torso surface and the actual breast. The volume estimated by the proposed method was found to be within the intra-operator variability among five segmentation trials performed manually by an expert on 3D torso scans of three subjects.},
author = {Chen, D. and Chittajallu, D. R. and Passalis, G. and Kakadiaris, I. A.},
doi = {10.1007/s10439-010-9971-z},
issn = {00906964},
journal = {Annals of Biomedical Engineering},
keywords = {Breast morphometry,Breast plastic surgery,Breast reconstruction,Breast shape model,Breast volume estimation,Computer-aided Surgery,Deformable model,Surgical planning},
pages = {1703--1718},
pmid = {20300850},
title = {{Computational tools for quantitative breast morphometry based on 3D scans}},
volume = {38},
year = {2010}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {9780387310732},
issn = {10179909},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
volume = {4},
year = {2006}
}
@inproceedings{Yalamanchili2010,
abstract = {The diaphragm is a thin double-domed muscle that separates the thoracic and abdominal cavities. An accurate delineation of the diaphragm surface will be useful in providing a good region of interest for segmentation problems pertaining to the thoracic and abdominal cavities. In this paper, we present a fully automatic 3D graph-based method for the segmentation of the diaphragm in non-contrast CT data. In particular, we reformulate the diaphragm segmentation problem as an optimal surface segmentation problem in a volumetric graph. Comparison of the results obtained using our method with manual segmentations performed by an expert on non-contrast cardiac CT scans of 7 randomly selected patients indicated an overlap of 94.20 \&amp;\#x00B1; 0.01\%.},
author = {Yalamanchili, Raja and Chittajallu, Deepak and Balanca, Paul and Tamarappoo, Balaji and Berman, Daniel and Dey, Damini and Kakadiaris, Ioannis},
booktitle = {2010 7th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, ISBI 2010 - Proceedings},
doi = {10.1109/ISBI.2010.5490132},
isbn = {9781424441266},
issn = {1945-7928},
keywords = {Diaphragm segmentation,Max closure,Non-contrast CT},
pages = {900--903},
pmid = {20513619},
title = {{Automatic segmentation of the diaphragm in non-contrast CT images}},
year = {2010}
}
@inproceedings{Agrawal2010,
abstract = {Conventional low frame rate cameras result in blur and/or aliasing in images while capturing fast dynamic events. Multiple low speed cameras have been used previously with staggered sampling to increase the temporal resolution. However, previous approaches are inefficient: they either use small integration time for each camera which does not provide light benefit, or use large integration time in a way that requires solving a big ill-posed linear system. We propose coded sampling that address these issues: using N cameras it allows N times temporal superresolution while allowing \~{}N/2 times more light compared to an equivalent high speed camera. In addition, it results in a well-posed linear system which can be solved independently for each frame, avoiding reconstruction artifacts and significantly reducing the computational time and memory. Our proposed sampling uses optimal multiplexing code considering additive Gaussian noise to achieve the maximum possible SNR in the recovered video. We show how to implement coded sampling on off-the-shelf machine vision cameras. We also propose a new class of invertible codes that allow continuous blur in captured frames, leading to an easier hardware implementation.},
author = {Agrawal, Amit and Gupta, Mohit and Veeraraghavan, Ashok and Narasimhan, Srinivasa G.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
issn = {10636919},
pages = {599--606},
title = {{Optimal coded sampling for temporal super-resolution}},
year = {2010}
}
@article{Cleveland1990,
abstract = {STL is a filtering procedure for decomposing a time series into trend, seasonal, and remainder components. STL has a simple design that consists of a sequence of applications of the loess smoother; the simplicity allows analysis of the properties of the procedure and allows fast computation, even for very long time series and large amounts of trend and seasonal smoothing. Other features of STL are specification of amounts of seasonal and trend smoothing that range, in a nearly continuous way, from a very small amount of smoothing to a very large amount; robust estimates of the trend and seasonal components that are not distorted by aberrant behavior in the data; specification of the period of the seasonal component to any integer multiple of the time sampling interval greater than one; and the ability to decompose time series with missing values.},
author = {Cleveland, Robert B and Cleveland, William S and McRae, Jean E and Terpenning, Irma},
file = {:home/cdeepakroy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cleveland et al. - 1990 - STL A seasonal-trend decomposition procedure based on loess.pdf:pdf},
issn = {0282-423X},
journal = {Journal of Official Statistics},
number = {1},
pages = {3--73},
title = {{STL: A seasonal-trend decomposition procedure based on loess}},
volume = {6},
year = {1990}
}
@inproceedings{Chittajallu2009a,
abstract = {The inner thoracic region consists of several important anatomical structures and an accurate delineation of this region is an essential step for various biomedical image analysis applications. In this paper, we present a fully automatic graph-based method for the delineation of the inner thoracic region in non-contrast cardiac CT data. In particular, we reformulate the problem of delineating the inner thoracic region as an optimal surface segmentation problem, the solution to which is obtained by computing the minimum-cost closed set in a node-weighted directed graph. Comparing the results obtained using our method with manual segmentations performed by an expert on non-contrast cardiac CT scans of 20 randomly selected patients indicated an overlap of 99.1 +/- 0.2\%.},
author = {Chittajallu, D. R. and Balanca, P. and Kakadiaris, I. A.},
booktitle = {Proceedings of the 31st Annual International Conference of the IEEE Engineering in Medicine and Biology Society: Engineering the Future of Biomedicine, EMBC 2009},
doi = {10.1109/IEMBS.2009.5332585},
isbn = {9781424432967},
issn = {1557-170X},
pages = {3569--3572},
pmid = {19963591},
title = {{Automatic delineation of the inner thoracic region in non-contrast CT data}},
year = {2009}
}
@article{Pace2013,
author = {Pace, Danielle F and Aylward, Stephen R and Niethammer, Marc},
file = {:home/cdeepakroy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pace, Aylward, Niethammer - 2013 - A Locally Adaptive Regularization Based on Anisotropic Diffusion for Deformable Image Registration of.pdf:pdf},
journal = {IEEE Transactions on Medical Imaging},
keywords = {abdominal computed tomography,ct,deformable image registration,locally adaptive,regularization,respiratory motion,sliding motion,thoracic ct},
number = {11},
pages = {2114--2126},
title = {{A Locally Adaptive Regularization Based on Anisotropic Diffusion for Deformable Image Registration of Sliding Organs}},
volume = {32},
year = {2013}
}
@inproceedings{Makihara2011,
abstract = {This paper describes a method for temporal super resolution from a single quasi-periodic image sequence. A so-called reconstruction-based method is applied to construct a one period image sequence with high frame-rate based on phase registration data in sub-frame order among multiple periods of the image sequence. First, the periodic image sequence to be reconstructed is expressed as a manifold in the parametric eigenspace of the phase. Given an input image sequence, phase registration and manifold reconstruction are alternately executed iteratively within an energy minimization framework that considers data fitness and the smoothness of both the manifold and the phase evolution. The energy minimization problem is solved through three-step coarse-to-fine procedures to avoid local minima. The experiments using both simulated and real data confirm the realization of temporal super resolution from a single image sequence.},
author = {Makihara, Yasushi and Mori, Atsushi and Yagi, Yasushi},
booktitle = {Computer Vision--ACCV 2010},
pages = {107--120},
title = {{Temporal super resolution from a single quasi-periodic image sequence based on phase registration}},
year = {2011}
}
@article{Mudenagudi2011,
abstract = {We address the problem of super-resolution—obtaining high-resolution images and videos from multiple low-resolution inputs. The increased resolution can be in spatial or temporal dimensions, or even in both. We present a unified framework which uses a generative model of the imaging process and can address spatial super-resolution, space-time super-resolution, image deconvolution, single-image expansion, removal of noise, and image restoration. We model a high-resolution image or video as a Markov random field and use maximum a posteriori estimate as the final solution using graph-cut optimization technique. We derive insights into what super-resolution magnification factors are possible and the conditions necessary for super-resolution. We demonstrate spatial super-resolution reconstruction results with magnifications higher than predicted limits of magnification. We also formulate a scheme for selective super-resolution reconstruction of videos to obtain simultaneous increase of resolutions in both spatial and temporal directions. We show that it is possible to achieve space-time magnification factors beyond what has been suggested in the literature by selectively applying super-resolution constraints. We present results on both synthetic and real input sequences.},
author = {Mudenagudi, Uma and Banerjee, Subhashis and Kalra, Prem Kumar},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Markov random field (MRF),Super-resolution,graph-cut,maximum a posteriori (MAP),minimization,nonlinear,space-time},
number = {5},
pages = {995--1008},
pmid = {20733227},
title = {{Space-time super-resolution using graph-cut optimization}},
volume = {33},
year = {2011}
}
@article{Lin2010,
archivePrefix = {arXiv},
arxivId = {math.OC/1009.5055},
author = {Lin, Z and Chen, M and Ma, Y},
eprint = {1009.5055},
journal = {ArXiv e-prints},
keywords = {Computer Science - Numerical Analysis,Computer Science - Systems and Control,Mathematics - Optimization and Control},
month = sep,
primaryClass = {math.OC},
title = {{The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices}},
year = {2010}
}
@inproceedings{Yalamanchili2010a,
abstract = {The diaphragm is a thin double-domed muscle that separates the thoracic and abdominal cavities. An accurate delineation of the diaphragm surface will be useful in providing a good region of interest for segmentation problems pertaining to the thoracic and abdominal cavities. In this paper, we present a fully automatic 3D graph-based method for the segmentation of the diaphragm in non-contrast CT data. In particular, we reformulate the diaphragm segmentation problem as an optimal surface segmentation problem in a volumetric graph. Comparison of the results obtained using our method with manual segmentations performed by an expert on non-contrast cardiac CT scans of 7 randomly selected patients indicated an overlap of 94.20 \&amp;\#x00B1; 0.01\%.},
author = {Yalamanchili, Raja and Chittajallu, Deepak and Balanca, Paul and Tamarappoo, Balaji and Berman, Daniel and Dey, Damini and Kakadiaris, Ioannis},
booktitle = {2010 7th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, ISBI 2010 - Proceedings},
keywords = {Diaphragm segmentation,Max closure,Non-contrast CT},
pages = {900--903},
pmid = {20513619},
title = {{Automatic segmentation of the diaphragm in non-contrast CT images}},
year = {2010}
}
@inproceedings{Niethammer2011,
author = {Niethammer, Marc and Hart, Gabriel L and Pace, Danielle F and Vespa, Paul M and Irimia, Andrei and Horn, John D Van and Aylward, Stephen R},
booktitle = {Medical Image Computing and Computer Assisted Intervention (MICCAI)},
file = {:home/cdeepakroy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Niethammer et al. - 2011 - Geometric Metamorphosis.pdf:pdf},
pages = {639--646},
title = {{Geometric Metamorphosis}},
year = {2011}
}
@article{Miller2001,
abstract = {This paper constructs metrics on the space of images I defined as orbits under group actions G. The groups studied include the finite dimensional matrix groups and their products, as well as the infinite dimensional diffeomorphisms examined in Trouv´ e (1999, Quaterly of Applied Math.) and Dupuis et al. (1998). Quaterly of Applied Math.). Left-invariant metrics are defined on the product G × I thus allowing the generation of transfor- mations of the background geometry as well as the image values. Examples of the application of such metrics are presented for rigid object matching with and without signature variation, curves and volume matching, and structural generation in which image values are changed supporting notions such as tissue creation in carrying one image to another.},
author = {Miller, M. I. and Younes, L.},
doi = {10.1023/A:1011161132514},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Groups of deformations,Image warping,Invariance,Riemannian metrics},
pages = {61--84},
title = {{Group actions, homeomorphisms, and matching: A general framework}},
volume = {41},
year = {2001}
}
@article{Cootney2001,
author = {Cootney, R. W.},
doi = {10.1093/ilar.42.3.233},
file = {:home/cdeepakroy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cootney - 2001 - Ultrasound Imaging Principles and Applications in Rodent Research.pdf:pdf},
issn = {1084-2020},
journal = {ILAR Journal},
keywords = {echocardiography,mice,rats,research,ultra-},
month = jan,
number = {3},
pages = {233--247},
title = {{Ultrasound Imaging: Principles and Applications in Rodent Research}},
url = {http://ilarjournal.oxfordjournals.org/cgi/doi/10.1093/ilar.42.3.233},
volume = {42},
year = {2001}
}
@article{Scholten2005,
abstract = {RATIONALE, AIMS AND OBJECTIVES: Technological progress in recent years has made it possible that ultrasound industry can now offer affordable, portable and battery-operated ultrasound systems the size of a laptop computer. The purpose of this study was to compare these hand-carried ultrasound instruments with standard echocardiography in order to investigate the facility of a rapid bedside diagnosis in patients with suspected or known cardiovascular disease. METHODS: Fifty consecutive patients were studied with miniaturized ultrasound equipment (SonoHeart) and a conventional scanner (Acuson Sequoia) in a blinded manner. All studies were performed by three board-certified cardiologists skilled and experienced in echocardiographic practice. Investigators were not aware of any previous medical reports. RESULTS: With the new system, adequate images could be obtained in all patients. Left ventricular and left atrial diameters measured with the hand-held system correlated well with those obtained with conventional scanning: r = 0.87, mean difference 3.12 +/- 2.7 mm and r = 0.84, mean difference 2.8 + 2.4 mm, respectively. The presence of left ventricular dysfunction, regional wall motion abnormalities, relevant valvular regurgitation (moderate or more) or valve stenosis was correctly diagnosed in all patients. However, there was a tendency towards underestimating the extent of wall motion abnormalities particularly in patients difficult to image. Discrepancies also frequently occurred in patients with trivial or mild regurgitation, where false-positive and false-negative findings were described. CONCLUSION: Currently available hand-held echocardiography systems can facilitate rapid bedside diagnosis and patient screening. However, this recent development in echocardiography also raises a number of questions and its actual impact on general clinical practice still remains to be evaluated.},
author = {Scholten, Christine and Rosenhek, Raphael and Binder, Thomas and Zehetgruber, Manfred and Maurer, Gerald and Baumgartner, Helmut},
doi = {10.1111/j.1365-2753.2004.00506.x},
isbn = {1356-1294 (Print)},
issn = {13561294},
journal = {Journal of Evaluation in Clinical Practice},
keywords = {Hand-held echocardiography,New technologies,Ultrasound devices},
pages = {67--72},
pmid = {15660539},
title = {{Hand-held miniaturized cardiac ultrasound instruments for rapid and effective bedside diagnosis and patient screening}},
volume = {11},
year = {2005}
}
@inproceedings{Akae2012,
abstract = {In this paper, we propose a temporal super resolution ap- proach for quasi-periodic image sequence such as human gait. The proposed method effectively combines example- based and reconstruction-based temporal super resolution approaches. A periodic image sequence is expressed as a manifold parameterized by a phase and a standard mani- fold is learned from multiple high frame-rate sequences in the training stage. In the test stage, an initial phase for each frame of an input low frame-rate image sequence is estimated based on the standard manifold at first, and the manifold reconstruction and the phase estimation are then iterated to generate better high frame-rate images in the energy minimization framework that ensures the fitness to both the input images and the standard manifold. The pro- posed method is applied to low frame-rate gait recognition and experiments with real data of 100 subjects demonstrate a significant improvement by the proposed method, particu- larly for quite low frame-rate videos (e.g., 1 fps).},
author = {Akae, Naoki and Mansur, Al and Makihara, Yasushi and Yagi, Yasushi},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1537--1543},
title = {{Video from nearly still: An application to low frame-rate gait recognition}},
year = {2012}
}
@article{Chatterjee2010,
abstract = {Image denoising has been a well studied problem in the field of image processing. Yet researchers continue to focus attention on it to better the current state-of-the-art. Recently proposed methods take different approaches to the problem and yet their denoising performances are comparable. A pertinent question then to ask is whether there is a theoretical limit to denoising performance and, more importantly, are we there yet? As camera manufacturers continue to pack increasing numbers of pixels per unit area, an increase in noise sensitivity manifests itself in the form of a noisier image. We study the performance bounds for the image denoising problem. Our work in this paper estimates a lower bound on the mean squared error of the denoised result and compares the performance of current state-of-the-art denoising methods with this bound. We show that despite the phenomenal recent progress in the quality of denoising algorithms, some room for improvement still remains for a wide class of general images, and at certain signal-to-noise levels. Therefore, image denoising is not dead--yet.},
author = {Chatterjee, Priyam and Milanfar, Peyman},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Bayesian Cram\&r\&Rao lower bound (CRLB),Bias,Bootstrapping,Image denoising,Mean squared error},
number = {4},
pages = {895--911},
pmid = {19932997},
title = {{Is denoising dead?}},
volume = {19},
year = {2010}
}
@misc{Schermelleh2010,
abstract = {For centuries, cell biology has been based on light microscopy and at the same time been limited by its optical resolution. However, several new technologies have been developed recently that bypass this limit. These new super-resolution technologies are either based on tailored illumination, nonlinear fluorophore responses, or the precise localization of single molecules. Overall, these new approaches have created unprecedented new possibilities to investigate the structure and function of cells.},
author = {Schermelleh, Lothar and Heintzmann, Rainer and Leonhardt, Heinrich},
booktitle = {Journal of Cell Biology},
number = {2},
pages = {165--175},
title = {{A guide to super-resolution fluorescence microscopy}},
volume = {190},
year = {2010}
}
@article{Kurkure2010,
abstract = {Accurate quantification of coronary artery calcium provides an opportunity to assess the extent of atherosclerosis disease. Coronary calcification burden has been reported to be associated with cardiovascular risk. Currently, an observer has to identify the coronary calcifications among a set of candidate regions, obtained by thresholding and connected component labeling, by clicking on them. To relieve the observer of such a labor-intensive task, an automated tool is needed that can detect and quantify the coronary calcifications. However, the diverse and heterogeneous nature of the candidate regions poses a significant challenge. In this paper, we investigate a supervised classification-based approach to distinguish the coronary calcifications from all the candidate regions and propose a two-stage, hierarchical classifier for automated coronary calcium detection. At each stage, we learn an ensemble of classifiers where each classifier is a cost-sensitive learner trained on a distinct asymmetrically sampled data subset. We compute the relative location of the calcifications with respect to a heart-centered coordinate system, and also use the neighboring regions of the calcifications to better characterize their properties for discrimination. Our method detected coronary calcifications with an accuracy, sensitivity and specificity of 98.27, 92.07 and 98.62\%, respectively, for a testing dataset of non-contrast computed tomography scans from 105 subjects.},
author = {Kurkure, Uday and Chittajallu, Deepak R. and Brunner, Gerd and Le, Yen H. and Kakadiaris, Ioannis A.},
doi = {10.1007/s10554-010-9607-2},
issn = {15695794},
journal = {International Journal of Cardiovascular Imaging},
keywords = {Computed tomography,Coronary calcium,Supervised classification},
pages = {817--828},
pmid = {20229312},
title = {{A supervised classification-based method for coronary calcium detection in non-contrast CT}},
volume = {26},
year = {2010}
}
@inproceedings{Brunner2008,
abstract = {There is growing evidence that calcified arterial deposits play a crucial role in the pathogenesis of cardiovascular disease. This paper investigates the challenging problem of unsupervised calcified lesion classification. We propose an algorithm, US-CALC (UnSupervised Calcified Arterial Lesion Classification), that discriminates arterial lesions from non-arterial lesions. The proposed method first mines the characteristics of calcified lesions using a novel optimization criterion and then identifies a subset of lesion features which is optimal for classification. Second, a two stage clustering is deployed to discriminate between arterial and non-arterial lesions. A histogram intersection distance measure is incorporated to determine cluster proximity. The clustering hierarchies are carefully validated and the final clusters are determined by a new intracluster compactness measure. Experimental results indicate an average accuracy of approximately 80\% on a database of electron beam CT heart scans.},
author = {Brunner, Gerd and Kurkure, Uday and Chittajallu, Deepak R. and Yalamanchili, Raja P. and Kakadiaris, Ioannis A.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-85988-8\_18},
isbn = {354085987X},
issn = {03029743},
pages = {144--152},
pmid = {18979742},
title = {{Toward unsupervised classification of calcified arterial lesions}},
volume = {5241 LNCS},
year = {2008}
}
@article{Kuybeda2013,
abstract = {The limitation of using low electron doses in non-destructive cryo-electron tomography of biological specimens can be partially offset via averaging of aligned and structurally homogeneous subsets present in tomograms. This type of sub-volume averaging is especially challenging when multiple species are present. Here, we tackle the problem of conformational separation and alignment with a "collaborative" approach designed to reduce the effect of the "curse of dimensionality" encountered in standard pair-wise comparisons. Our new approach is based on using the nuclear norm as a collaborative similarity measure for alignment of sub-volumes, and by exploiting the presence of symmetry early in the processing. We provide a strict validation of this method by analyzing mixtures of intact simian immunodeficiency viruses SIV mac239 and SIV CP-MAC. Electron microscopic images of these two virus preparations are indistinguishable except for subtle differences in conformation of the envelope glycoproteins displayed on the surface of each virus particle. By using the nuclear norm-based, collaborative alignment method presented here, we demonstrate that the genetic identity of each virus particle present in the mixture can be assigned based solely on the structural information derived from single envelope glycoproteins displayed on the virus surface.},
archivePrefix = {arXiv},
arxivId = {1009.5055},
author = {Kuybeda, Oleg and Frank, Gabriel A and Bartesaghi, Alberto and Borgnia, Mario and Subramaniam, Sriram and Sapiro, Guillermo},
doi = {10.1016/j.jsb.2012.10.010},
eprint = {1009.5055},
file = {:home/cdeepakroy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuybeda et al. - 2013 - A collaborative framework for 3D alignment and classification of heterogeneous subvolumes in cryo-electron tomog.pdf:pdf},
issn = {1095-8657},
journal = {Journal of structural biology},
keywords = {Algorithms,Cryoelectron Microscopy,Cryoelectron Microscopy: methods,Electron Microscope Tomography,Electron Microscope Tomography: methods,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Protein Conformation,Simian immunodeficiency virus,Simian immunodeficiency virus: ultrastructure,Viral Envelope Proteins,Viral Envelope Proteins: ultrastructure},
month = feb,
number = {2},
pages = {116--27},
pmid = {23110852},
title = {{A collaborative framework for 3D alignment and classification of heterogeneous subvolumes in cryo-electron tomography.}},
url = {http://arxiv.org/abs/1009.5055},
volume = {181},
year = {2013}
}
@article{Beaulieu2007,
abstract = {Advances in ultrasound technology continue to enhance its diagnostic applications in daily medical practice. Bedside echocardiographic examination has become useful to properly trained cardiologists, anesthesiologists, intensivists, surgeons, and emergency room physicians. Cardiac ultrasound can permit rapid, accurate, and noninvasive diagnosis of a broad range of acute cardiovascular pathologies. Although transesophageal echocardiography was once the principal diagnostic approach using ultrasound to evaluate intensive care unit patients, advances in ultrasound imaging, including harmonic imaging, digital acquisition, and contrast for endocardial enhancement, has improved the diagnostic yield of transthoracic echocardiography. Ultrasound devices continue to become more portable, and hand-carried devices are now readily available for bedside applications. This article discusses the application of bedside echocardiography in the intensive care unit. The emphasis is on echocardiography and cardiovascular diagnostics, specifically on goal-directed bedside cardiac ultrasonography.},
author = {Beaulieu, Yanick},
doi = {10.1097/01.CCM.0000260673.66681.AF},
isbn = {0000260673},
issn = {0090-3493},
journal = {Critical care medicine},
pages = {S235--S249},
pmid = {17446784},
title = {{Bedside echocardiography in the assessment of the critically ill.}},
volume = {35},
year = {2007}
}
@article{Burges2009,
author = {Burges, Christopher J C},
doi = {10.1561/2200000002},
issn = {1935-8237},
journal = {Foundations and Trends® in Machine Learning},
keywords = {Dimensionality reduction},
number = {4},
pages = {275--365},
publisher = {Now Publishers},
title = {{Dimension Reduction: A Guided Tour}},
volume = {2},
year = {2009}
}
@inproceedings{Shahar2011,
abstract = {Spatial Super Resolution (SR) aims to recover fine image details, smaller than a pixel size. Temporal SR aims to recover rapid dynamic events that occur faster than the video frame-rate, and are therefore invisible or seen incorrectly in the video sequence. Previous methods for Space-Time SR combined information from multiple video recordings of the same dynamic scene. In this paper we show how this can be done from a single video recording. Our approach is based on the observation that small space-time patches (\&\#x2018;ST-patches\&\#x2019;, e.g., 5\&\#x00D7;5\&\#x00D7;3) of a single \&\#x2018;natural video\&\#x2019;, recur many times inside the same video sequence at multiple spatio-temporal scales. We statistically explore the degree of these ST-patch recurrences inside \&\#x2018;natural videos\&\#x2019;, and show that this is a very strong statistical phenomenon. Space-time SR is obtained by combining information from multiple ST-patches at sub-frame accuracy. We show how finding similar ST-patches can be done both efficiently (with a randomized-based search in space-time), and at sub-frame accuracy (despite severe motion aliasing). Our approach is particularly useful for temporal SR, resolving both severe motion aliasing and severe motion blur in complex \&\#x2018;natural videos\&\#x2019;.},
author = {Shahar, Oded and Faktor, Alon and Irani, Michal},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
issn = {10636919},
pages = {3353--3360},
title = {{Space-time super-resolution from a single video}},
year = {2011}
}
@article{Wiley2014,
abstract = {In 1903, Dr. William Osler advocated for reform of medical education to emphasize bedside teaching, recommending “no teaching without a patient for a text and the best teaching is that taught by the patient himself” (1). More than a century later, new voices in the profession echo that sentiment, suggesting that diagnosis has again strayed from the bedside. Some propose that technology has usurped the clinical examination at the expense of patient care and the cognitive development of practitioners. Proponents of bedside medicine lament that ward rounds have been reduced to examining a patient's electronic medical record and clicking computerized order sets based on results of myriad prior diagnostic tests.},
annote = {10.1016/j.jacc.2014.05.011},
author = {Wiley, Brandon and Mohanty, Bibhu},
issn = {0735-1097},
journal = {Journal of the American College of Cardiology},
month = jul,
number = {2},
pages = {229--230},
title = {{Handheld Ultrasound and Diagnosis of Cardiovascular Disease at the Bedside}},
volume = {64},
year = {2014}
}
@article{Lu2013,
abstract = {The instantaneous phase estimated by the Hilbert transform (HT) is susceptible to noise; we propose a robust approach for the estimation of instantaneous phase in noisy situations. The main procedure of the proposed method is applying an adaptive filter in time-frequency domain and calculating the analytic signal. By supposing that one frequency component with higher amplitude has higher signal-to-noise ratio, a zero-phase adaptive filter, which is constructed by using the time-frequency amplitude spectrum, enhances the frequency components with higher amplitudes and suppresses those with lower amplitudes. The estimation of instantaneous frequency, which is defined as the derivative of instantaneous phase, is also improved by the proposed robust instantaneous phase estimation method. Synthetic and field data sets are used to demonstrate the performance of the proposed method for the estimation of instantaneous phase and frequency, compared by the HT and short-time-Fourier-transform methods. © 2012 Society of Exploration Geophysicists.},
author = {Lu, Wen-kai and Zhang, Chang-Kai},
doi = {10.1190/geo2011-0435.1},
issn = {0016-8033},
journal = {GEOPHYSICS},
number = {1},
pages = {O1--O7},
title = {{Robust estimation of instantaneous phase using a time-frequency adaptive filter}},
volume = {78},
year = {2013}
}
@article{Candes2009,
abstract = {This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the L1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
archivePrefix = {arXiv},
arxivId = {arXiv:0912.3599v1},
author = {Cand\`{e}s, Emmanuel J and Li, Xiaodong and Ma, Yi and Wright, John and Candes, Emmanuel J},
eprint = {arXiv:0912.3599v1},
institution = {Department of Statistics, Stanford University},
isbn = {0-7803-6278-0},
issn = {0004-5411},
journal = {Journal of the ACM},
keywords = {\&ell,1-norm minimization,Principal components,duality,low-rank matrices,nuclear-norm minimization,robustness vis-a-vis outliers,sparsity,video surveillance},
number = {3},
pages = {1--37},
publisher = {ACM Request Permissions},
title = {{Robust Principal Component Analysis?}},
volume = {58},
year = {2009}
}
@article{Chittajallu2014,
abstract = {Image segmentation is, in general, an ill-posed problem and additional constraints need to be imposed in order to achieve the desired segmentation result. While segmenting organs in medical images, which is the topic of this paper, a significant amount of prior knowledge about the shape, appearance, and location of the organs is available that can be used to constrain the solution space of the segmentation problem. Among the various types of prior information, the incorporation of prior information about shape, in particular, is very challenging. In this paper, we present an explicit shape-constrained MAP-MRF-based contour evolution method for the segmentation of organs in 2-D medical images. Specifically, we represent the segmentation contour explicitly as a chain of control points. We then cast the segmentation problem as a contour evolution problem, wherein the evolution of the contour is performed by iteratively solving a MAP-MRF labeling problem. The evolution of the contour is governed by three types of prior information, namely: (i) appearance prior, (ii) boundary-edgeness prior, and (iii) shape prior, each of which is incorporated as clique potentials into the MAP-MRF problem. We use the master-slave dual decomposition framework to solve the MAP-MRF labeling problem in each iteration. In our experiments, we demonstrate the application of the proposed method to the challenging problem of heart segmentation in non-contrast computed tomography data.},
author = {Chittajallu, Deepak R. and Paragios, Nikos and Kakadiaris, Ioannis A.},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Contour evolution,Markov random field model,Medical image segmentation,Shape priors},
number = {1},
pages = {120--129},
pmid = {24403409},
title = {{An explicit shape-constrained MRF-based contour evolution method for 2-D medical image segmentation}},
volume = {18},
year = {2014}
}
@article{Cutler2000,
abstract = {We describe new techniques to detect and analyze periodic motion as seen from both a static and a moving camera. By tracking objects of interest, we compute an object's self-similarity as it evolves in time. For periodic motion, the self-similarity measure is also periodic and we apply time-frequency analysis to detect and characterize the periodic motion. The periodicity is also analyzed robustly using the 2D lattice structures inherent in similarity matrices. A real-time system has been implemented to track and classify objects using periodicity. Examples of object classification (people, running dogs, vehicles), person counting,},
author = {Cutler, R and Davis, L S},
doi = {10.1109/CVPR.1999.784652},
isbn = {0769501494},
issn = {10636919},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {781--796},
title = {{Robust real-time periodic motion detection, analysis, and applications}},
volume = {22},
year = {2000}
}
@article{Jones2011,
abstract = {We report super-resolution fluorescence imaging of live cells with high spatiotemporal resolution using stochastic optical reconstruction microscopy (STORM). By labeling proteins either directly or via SNAP tags with photoswitchable dyes, we obtained two-dimensional (2D) and 3D super-resolution images of living cells, using clathrin-coated pits and the transferrin cargo as model systems. Bright, fast-switching probes enabled us to achieve 2D imaging at spatial resolutions of ∼25 nm and temporal resolutions as fast as 0.5 s. We also demonstrated live-cell 3D super-resolution imaging. We obtained 3D spatial resolution of ∼30 nm in the lateral direction and ∼50 nm in the axial direction at time resolutions as fast as 1-2 s with several independent snapshots. Using photoswitchable dyes with distinct emission wavelengths, we also demonstrated two-color 3D super-resolution imaging in live cells. These imaging capabilities open a new window for characterizing cellular structures in living cells at the ultrastructural level.},
author = {Jones, Sara A and Shim, Sang-Hee and He, Jiang and Zhuang, Xiaowei},
institution = {Department of Chemistry and Chemical Biology, Harvard University, Cambridge, Massachusetts, USA.},
journal = {Nature methods},
number = {6},
pages = {499--508},
title = {{Fast, three-dimensional super-resolution imaging of live cells.}},
volume = {8},
year = {2011}
}
@article{Shechtman2005,
abstract = {We propose a method for constructing a video sequence of high space-time resolution by combining information from multiple low-resolution video sequences of the same dynamic scene. Super-resolution is performed simultaneously in time and in space. By "temporal super-resolution," we mean recovering rapid dynamic events that occur faster than regular frame-rate. Such dynamic events are not visible (or else are observed incorrectly) in any of the input sequences, even if these are played in "slow-motion." The spatial and temporal dimensions are very different in nature, yet are interrelated. This leads to interesting visual trade-offs in time and space and to new video applications. These include: 1) treatment of spatial artifacts (e.g., motion-blur) by increasing the temporal resolution and 2) combination of input sequences of different space-time resolutions (e.g., NTSC, PAL, and even high quality still images) to generate a high quality video sequence. We further analyze and compare characteristics of temporal super-resolution to those of spatial super-resolution. These include: How many video cameras are needed to obtain increased resolution? What is the upper bound on resolution improvement via super-resolution? What is the temporal analogue to the spatial "ringing" effect?},
author = {Shechtman, Eli and Caspi, Yaron and Irani, Michal},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Fast cameras,High-quality video,Motion aliasing,Motion blur,Space-time analysis,Super-resolution,Temporal resolution},
number = {4},
pages = {531--545},
pmid = {15794159},
title = {{Space-time super-resolution}},
volume = {27},
year = {2005}
}
@article{Nasrollahi2014,
author = {Nasrollahi, Kamal and Moeslund, ThomasB.},
doi = {10.1007/s00138-014-0623-4},
issn = {0932-8092},
journal = {Machine Vision and Applications},
keywords = {Hallucination,Reconstruction,Regularization,Super-resolution},
language = {English},
number = {6},
pages = {1423--1468},
publisher = {Springer Berlin Heidelberg},
title = {{Super-resolution: a comprehensive survey}},
volume = {25},
year = {2014}
}
@article{Makihara2014,
abstract = {We propose a method for phase estimation of a single non-parametric quasi-periodic signal. Assuming signal intensities should be equal among samples of the same phase, such corresponding samples are obtained by self-dynamic time warping between a quasi-periodic signal and a signal with multiple-period shifts applied. A phase sequence is then estimated in a sub-sampling order using an optimization framework incorporating 1) a data term derived from the correspondences and 2) a smoothness term of the local phase evolution under 3) a monotonic-increasing constraint on the phase. Such a phase estimation is, however, ill-posed because of combination ambiguity between the phase evolution and the normalized periodic signal, and hence can result in a biased solution. Therefore, we introduce into the optimization framework 4) a bias correction term, which imposes zero-bias from the linear phase evolution. Analysis of the quasi-periodic signals from both simulated and real data indicate the effectiveness and also potential applications of the proposed method.},
author = {Makihara, Yasushi and Aqmar, Muhammad Rasyid and Trung, Ngo Thanh and Nagahara, Hajime and Sagawa, Ryusuke and Mukaigawa, Yasuhiro and Yagi, Yasushi},
journal = {IEEE Transactions on Signal Processing},
keywords = {Phase,dynamic time warping,quasi-periodic signal},
number = {8},
pages = {2066--2079},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Phase estimation of a single quasi-periodic signal}},
volume = {62},
year = {2014}
}
